{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jason910315/fine-tuning-llm-model/blob/main/fine_tuning_fortune_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unsloth 是一個用於大型語言模型微調的工具，可以讓模型運行更快，佔用更少內存\n",
        "!pip install unsloth\n",
        "\n",
        "# 卸載當前已安装的 unsloth 包（如果已安装），然後從 Github 安裝最新版本。\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "# bitsandbytes 是一個用於量化和優化模型的工具，幫著減少模型占用的內存\n",
        "# unsloth_zoo 包含了一些預訓練模型或其他工具。\n",
        "!pip install bitsandbytes unsloth_zoo"
      ],
      "metadata": {
        "id": "np9yDjYqn8-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load pretrained model"
      ],
      "metadata": {
        "id": "xsPgq86zqcWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048  # 模型處理上下文的最大長度\n",
        "dtype = None           # 資料類型，讓模型選擇最適合的精度\n",
        "load_in_4bit = True    # 使用 4bits 減少模型精度以節省內存\n",
        "\n",
        "# 載入預訓練模型，並獲得 tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit\n",
        ")"
      ],
      "metadata": {
        "id": "uDQMV6B82NNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "mZCC1wkGrfC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定義提示風格\n",
        "prompt_style = \"\"\"以下是描述任務的指令，以及提供進一步上下文的輸入。\n",
        "請寫出一個適當完成請求的回答。\n",
        "在回答之前，請仔細思考問題，並創建一個邏輯連貫的思考過程，以確保回答準確無誤。\n",
        "\n",
        "### 指令：\n",
        "你是一位精通卜卦、星象和運勢預測的算命大師。\n",
        "請回答以下算命問題\n",
        "\n",
        "### 問題：\n",
        "{}\n",
        "\n",
        "### 回答：\n",
        "<think>{}\"\"\"\n",
        "question = \"1992年閏年四月初九巳时生人，女，想了解健康運勢\""
      ],
      "metadata": {
        "id": "ugNJ-UIfrgll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備推理模型\n",
        "FastLanguageModel.for_inference(model)\n",
        "# 把 question、空字串傳入 template 的空格，並將文字轉成模型可以吃的張量輸入\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# 讓模型根據問題生成回答，最多 1200 個詞\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,  # 輸入數字序列\n",
        "    attention_mask=inputs.attention_mask,  # 註意力遮罩，讓模型知道哪部分更重要\n",
        "    max_new_tokens=1200,\n",
        "    use_cache=True,  # 使用緩存加速生成\n",
        ")\n",
        "\n",
        "# 解碼模型輸出為可讀文本\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0])"
      ],
      "metadata": {
        "id": "0ItSO5Bk2pSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load dataset"
      ],
      "metadata": {
        "id": "2w2Oj1Lm8yX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 定義格式化提示的字串，用於微調的模板(提示詞將模型角色的形容更加精準)\n",
        "train_prompt_style = \"\"\"以下是描述任務的指令，以及提供進一步上下文的輸入。\n",
        "請寫出一個適當完成請求的回答。\n",
        "在回答之前，請仔細思考問題，並創建一個邏輯連貫的思考過程，以確保回答準確無誤。\n",
        "\n",
        "### 指令：\n",
        "你是一位精通八字算命、 紫微斗數、 風水、易經卦象、塔羅牌占卜、星象、面相手相和運勢預測等方面的算命大师。\n",
        "請回答以下算命問題\n",
        "\n",
        "### 問題：\n",
        "{}\n",
        "\n",
        "### 回答：\n",
        "<思考>\n",
        "{}\n",
        "</思考>\n",
        "{}\"\"\"\n"
      ],
      "metadata": {
        "id": "T2__pF0Z10ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "from datasets import load_dataset\n",
        "# 以前 200 筆數據作為訓練，並使用預設的語言(default)\n",
        "dataset = load_dataset(\"Conard/fortune-telling\", 'default', split = \"train[0:200]\", trust_remote_code=True)\n",
        "print(dataset.column_names)"
      ],
      "metadata": {
        "id": "9wIj2c379LRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 格式化資料集中的每筆記錄\n",
        "def formatting_prompts_func(examples):\n",
        "  # 從資料集提取問題、思考過程、以及回答\n",
        "  inputs = examples[\"Question\"]\n",
        "  cots = examples[\"Complex_CoT\"]\n",
        "  outputs = examples[\"Response\"]\n",
        "  texts = []  # 儲存格式化後的文本，用於輸入到微調\n",
        "  # 將每個問題、思考過程、回答都嵌入提示詞模板，並輸入到 texts\n",
        "  for input, cot, output in zip(inputs, cots, outputs):\n",
        "    # 最後為每個紀錄加上結尾\n",
        "    text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "  # 返回所有格式化文本的字典\n",
        "  return {\n",
        "      \"text\": texts,\n",
        "  }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "dataset[\"text\"][0]  # 印出第一筆記錄來觀察\n"
      ],
      "metadata": {
        "id": "XD6EVDaR9K09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning"
      ],
      "metadata": {
        "id": "xoLRDlP1S7Be"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_training(model)\n",
        "\n",
        "# 使用 Parameter-Efficient Fine Tuning，並將模型轉為準備微調的狀態\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,     # 已經加載過的預訓練模型\n",
        "    r = 16,     # LoRA 的的秩，決定可訓練的參數量\n",
        "    # 指定模型中需要微調的區塊，包括 Attention 的 query、key、 value、 output\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,  # 設定 LoRA 的超參數\n",
        "    lora_dropout = 0, # 設定參數丟棄綠\n",
        "    bias = \"none\",    # 是否添加偏置項\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "iDvuUnvMS94f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer  # SFFTaininer，用於監督式微調\n",
        "from transformers import TrainingArguments  # 用於設定訓練參數\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# 創建一個 SFFTrainer 物件\n",
        "trainer = SFTTrainer(\n",
        "    model=model,   # 傳入要微調的模型\n",
        "    tokenizer=tokenizer,  # 用於處理文本數據\n",
        "    train_dataset=dataset,  # 訓練資料集\n",
        "    dataset_text_field=\"text\",  # 指定訓練資料集中的文本標題 (上面定義過的)\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,    # 資料處理的並行數\n",
        "    packing=False,\n",
        "    # 定義訓練參數\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,  # 每個設備 (如 GPU) 上的批量大小\n",
        "        warmup_steps=5,  # 預熱，訓練開始時學習綠開始增加的步數\n",
        "        max_steps=75,  # 最大訓練步數 (epoches)\n",
        "        learning_rate=2e-4,  # 學習率\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,  # 每隔多少步記錄一次日誌\n",
        "        optim=\"adamw_8bit\",  # 使用的優化器，用於調整模型參數\n",
        "        weight_decay=0.01,  # 權重衰減，防止過擬合\n",
        "        lr_scheduler_type=\"linear\",  # 控制學習率的變化方式\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "D4023kzmS40D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 開始 fine-tuning\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "jCTOsLQJNsO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Fine tuning"
      ],
      "metadata": {
        "id": "pz19oBvzPV0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 再次印出問體檢視\n",
        "print(question)"
      ],
      "metadata": {
        "id": "TtnmzaiKPh9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 將模型切換到推理模式，準備回答問題\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=4000,\n",
        "    use_cache=True,\n",
        ")\n",
        "\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0])"
      ],
      "metadata": {
        "id": "z1dpYPgwPaLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save model to GGUF"
      ],
      "metadata": {
        "id": "HYEb-dVrQpOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GGUF 是一種高效存取 LLM 模型的格式，支持多种量化方法 (4、8、16bit)，可以顯著減小模型的大小，便於儲存\n",
        "# 例如在 Ollama 上部署時，量化後的模型在資源受限的設備上運行更快。\n",
        "\n",
        "# 用於存取用戶資料\n",
        "from google.colab import userdata\n",
        "\n",
        "# 從 Colab 設定中獲得 huggingface 的密鑰\n",
        "HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "\n",
        "# 保存為 8bit 量化格式（Q8_0）\n",
        "# 這種格式文件小且運行快，適合部署到資源受限的設備\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "\n",
        "# 保存為 16bit 量化格式\n",
        "# 量化精度更高，但文件較大\n",
        "if False: model.save_pretrained_gguf(\"model_f16\", tokenizer, quantization_method = \"f16\")\n",
        "\n",
        "# 保存為 4bit 量化格式（q4_k_m）\n",
        "# 文件最小，但精度稍低\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")"
      ],
      "metadata": {
        "id": "k9ryCb3nPUio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload to HuggingFace"
      ],
      "metadata": {
        "id": "mdZoyIzSUhKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 用於創建新的 model repository\n",
        "from huggingface_hub import create_repo\n",
        "\n",
        "# 在 Hugging Face Hub 上創建一個新的倉庫\n",
        "create_repo(\"Jasonyang0315/fortunetelling_model\", token=HUGGINGFACE_TOKEN, exist_ok=True)\n",
        "\n",
        "# 將模型和 tokenizer 上傳\n",
        "model.push_to_hub_gguf(\"Jasonyang0315/fortunetelling_model\", tokenizer, token=HUGGINGFACE_TOKEN)"
      ],
      "metadata": {
        "id": "1wHgN_R2UYhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}